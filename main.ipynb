{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['NORMAL', 'PNEUMONIA']\n",
    "\n",
    "IMG_HEIGHT, IMG_WIDTH = 150, 150\n",
    "BATCH_SIZE = 16\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SEED = 42\n",
    "\n",
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'\n",
    "val_dir = 'data/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_num_files = len(list(Path(train_dir + '/' + classes[0]).glob('*.jpeg')))\n",
    "train_pneumonia_num_files = len(list(Path(train_dir + '/' + classes[1]).glob('*.jpeg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(\n",
    "    x=classes,\n",
    "    y=[train_normal_num_files, train_pneumonia_num_files],\n",
    "    hue=classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are almost 3 times more Pneumonia images than normal ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`image_dataset_from_directory` generates a tf.data.Dataset from image files in a directory.\n",
    "\n",
    "If your directory structure is:\n",
    "\n",
    "    main_directory/\n",
    "    ...class_a/\n",
    "    ......a_image_1.jpg\n",
    "    ......a_image_2.jpg\n",
    "    ...class_b/\n",
    "    ......b_image_1.jpg\n",
    "    ......b_image_2.jpg\n",
    "Then calling image_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 files belonging to 2 classes.\n",
      "Found 16 files belonging to 2 classes.\n",
      "Found 624 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED,\n",
    "    shuffle=True,\n",
    "    color_mode='rgb',\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    color_mode='rgb',\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    color_mode='rgb',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Benefits of Using the tf.keras.layers Augmentation Layer\n",
    "\n",
    "**1. On-the-fly augmentation during training**  \n",
    "- Augmentations are applied dynamically during training, which means:\n",
    "    - No extra disk space is needed for augmented images.\n",
    "    - The model sees new variations every epoch, improving generalization.\n",
    "- It helps simulate a larger, more diverse dataset without manual duplication.\n",
    "\n",
    "**2. GPU-accelerated and built into the TensorFlow graph**\n",
    "- These layers run as part of the TensorFlow computational graph, meaning:\n",
    "    - Theyâ€™re fast and optimized for GPUs/TPUs.\n",
    "    - No slow, CPU-bound Python loops or I/O bottlenecks.\n",
    "- Ideal for large-scale training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n"
     ]
    }
   ],
   "source": [
    "# Use Keras layers as augmentations\n",
    "# This is a simple augmentation pipeline that includes random flipping, rotation, zooming, and contrast adjustment.\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.05),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "])  \n",
    "\n",
    "def augment(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize the image to [0, 1]\n",
    "    image = data_augmentation(image)\n",
    "    return image, label\n",
    "\n",
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize the image to [0, 1]\n",
    "    return image, label\n",
    "\n",
    "# Apply the augmentation to the training dataset\n",
    "train_ds = train_ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Normalize the validation and test datasets\n",
    "val_ds = val_ds.map(normalize, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(normalize, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prefetch data for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prefetch()` allows the data loading and model training to run in parallel.\n",
    "\n",
    "Without prefetching, the workflow looks like this â€” sequential and slower:\n",
    "\n",
    "- Load a batch of data\n",
    "- Train on that batch\n",
    "- Load the next batch\n",
    "- Train again\n",
    "(Repeat...)\n",
    "\n",
    "This means the GPU waits while the CPU loads data â€” wasted time!\n",
    "\n",
    "With `prefetch(buffer_size=N)`, TensorFlow starts loading the next N batches in the background while the model is training on the current batch.\n",
    "So the steps overlap like this:\n",
    "\n",
    "- While training on batch A, the pipeline is loading batch B\n",
    "- When batch A is done, batch B is ready instantly  \n",
    "\n",
    "`tf.data.AUTOTUNE` lets TensorFlow decide the optimal buffer size dynamically.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetch for performance\n",
    "# This allows later batches to be prepared while the current one is being processed\n",
    "# This will speed up training\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Use Class Weights?**  \n",
    "When the dataset is imbalanced (e.g., many more \"Normal\" than \"Pneumonia\" X-rays), the model can get high accuracy by just predicting the majority class all the time.\n",
    "\n",
    "Imagine this:\n",
    "- 90% Normal\n",
    "- 10% Pneumonia\n",
    "\n",
    "A model that always predicts \"Normal\" would have 90% accuracy, but 0% recall for Pneumonia â€” which is unacceptable in medical diagnostics.\n",
    "\n",
    "So, to combat this imbalance, class weights adjust the importance of each class when calculating the loss.\n",
    "\n",
    "**What Do Class Weights Do, Mathematically?**  \n",
    "In a standard loss like binary cross-entropy, each example contributes equally. But with class weights, the contribution to the loss is scaled:\n",
    "\n",
    "Binary Cross-Entropy (with weights):  \n",
    "Loss = âˆ’ð‘¤1 â‹… ð‘¦ â‹… log(ð‘¦^) âˆ’ ð‘¤0 â‹… (1 âˆ’ ð‘¦) â‹… log(1 âˆ’ ð‘¦^)\n",
    "\n",
    "Where:\n",
    "- ð‘¦ is the true label\n",
    "- ð‘¦^ is the predicted probability\n",
    "- ð‘¤1, ð‘¤0 are weights for class 1 and 0, respectively\n",
    "\n",
    "This makes mistakes on minority class samples \"hurt more\" in training, nudging the model to learn their features better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 1.9448173005219984, 1: 0.6730322580645162}\n"
     ]
    }
   ],
   "source": [
    "# Get all training labels\n",
    "y_train = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(y_train),\n",
    "                                     y=y_train)\n",
    "\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 8192)              14714688  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1048704   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,763,521\n",
      "Trainable params: 5,768,449\n",
      "Non-trainable params: 9,995,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg = vgg16.VGG16(include_top=False,\n",
    "                        weights='imagenet',\n",
    "                        input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "output = vgg.output\n",
    "output = tf.keras.layers.Flatten()(output)\n",
    "\n",
    "basemodel = Model(vgg.input, output)\n",
    "# basemodel.trainable = False\n",
    "# for layer in basemodel.layers: layer.trainable = False \n",
    "basemodel.trainable = True\n",
    "for layer in basemodel.layers[:-4]:  # Fine-tune last few layers\n",
    "    layer.trainable = False\n",
    "\n",
    "input_shape = basemodel.output_shape[1]\n",
    "model = Sequential()\n",
    "model.add(basemodel)\n",
    "model.add(Dense(128, activation='relu', input_dim=input_shape))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              metrics=['accuracy', \n",
    "                       tf.keras.metrics.AUC(name='auc'), \n",
    "                    #    tf.keras.metrics.Precision(name='precision'), \n",
    "                    #    tf.keras.metrics.Recall(name='recall')\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', # You can also use 'val_auc' or 'val_accuracy'\n",
    "                               patience=4, # Wait n epochs before stopping if no improvement\n",
    "                               verbose=1,\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "326/326 [==============================] - 29s 65ms/step - loss: 0.2163 - accuracy: 0.9087 - auc: 0.9769 - val_loss: 0.1459 - val_accuracy: 0.9375 - val_auc: 1.0000\n",
      "Epoch 2/10\n",
      "326/326 [==============================] - 21s 64ms/step - loss: 0.0997 - accuracy: 0.9613 - auc: 0.9938 - val_loss: 0.1187 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 3/10\n",
      "326/326 [==============================] - 21s 64ms/step - loss: 0.0885 - accuracy: 0.9641 - auc: 0.9951 - val_loss: 0.1148 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 4/10\n",
      "326/326 [==============================] - 21s 64ms/step - loss: 0.0736 - accuracy: 0.9724 - auc: 0.9965 - val_loss: 0.1548 - val_accuracy: 0.9375 - val_auc: 1.0000\n",
      "Epoch 5/10\n",
      "326/326 [==============================] - 21s 64ms/step - loss: 0.0682 - accuracy: 0.9745 - auc: 0.9969 - val_loss: 0.1105 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/10\n",
      "326/326 [==============================] - 21s 65ms/step - loss: 0.0613 - accuracy: 0.9772 - auc: 0.9974 - val_loss: 0.1238 - val_accuracy: 0.9375 - val_auc: 1.0000\n",
      "Epoch 7/10\n",
      "326/326 [==============================] - 21s 62ms/step - loss: 0.0504 - accuracy: 0.9806 - auc: 0.9982 - val_loss: 0.1145 - val_accuracy: 0.9375 - val_auc: 1.0000\n",
      "Epoch 8/10\n",
      "326/326 [==============================] - 21s 64ms/step - loss: 0.0481 - accuracy: 0.9799 - auc: 0.9986 - val_loss: 0.0831 - val_accuracy: 0.9375 - val_auc: 1.0000\n",
      "Epoch 9/10\n",
      "326/326 [==============================] - 22s 65ms/step - loss: 0.0445 - accuracy: 0.9812 - auc: 0.9988 - val_loss: 0.1061 - val_accuracy: 0.9375 - val_auc: 1.0000\n",
      "Epoch 10/10\n",
      "326/326 [==============================] - 22s 66ms/step - loss: 0.0392 - accuracy: 0.9850 - auc: 0.9988 - val_loss: 0.0828 - val_accuracy: 1.0000 - val_auc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(train_ds, \n",
    "                    epochs=10,\n",
    "                    validation_data=val_ds,\n",
    "                    class_weight=class_weight_dict,\n",
    "                    callbacks=[early_stopping],\n",
    "                    batch_size=BATCH_SIZE,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 2s 51ms/step - loss: 0.3406 - accuracy: 0.9038 - auc: 0.9587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3406071364879608, 0.9038461446762085, 0.9586948156356812]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "images = []\n",
    "\n",
    "for batch_images, batch_labels in test_ds:\n",
    "    preds = model.predict(batch_images)\n",
    "    y_true.extend(batch_labels.numpy())\n",
    "    y_pred.extend((preds > 0.5).astype(int).flatten())\n",
    "    images.extend(batch_images.numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)   \n",
    "images = np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total misclassified images: 60\n"
     ]
    }
   ],
   "source": [
    "misclassified_idxs = np.where(y_true != y_pred)[0]  # Get the indices of misclassified images\n",
    "print(f\"Total misclassified images: {len(misclassified_idxs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal misclassified: 55\n",
      "Pneumonia misclassified: 5\n"
     ]
    }
   ],
   "source": [
    "normal_misclassified = np.sum(y_true[misclassified_idxs] == 0)\n",
    "pneumonia_misclassified = np.sum(y_true[misclassified_idxs] == 1)\n",
    "print(f\"Normal misclassified: {normal_misclassified}\")\n",
    "print(f\"Pneumonia misclassified: {pneumonia_misclassified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catboost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
